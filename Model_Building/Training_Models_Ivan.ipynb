{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read\n",
    "We read the cleaned accepted loans strictly before 2015. We train our models here. We will focus only on 'high' risk loans, that is loans with grade 'D' or below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted = pd.read_csv('/Users/ivanpassoni/Google Drive/LendingClubData/ml datasets/accepted_b_2015_ml.csv',\n",
    "                       low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted.drop(index = accepted.loc[accepted['loan_status']=='Current', :].index, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting high risk loans (Loans with grade not in ['A', 'B','C'])\n",
    "X = accepted.loc[accepted['sub_grade'].apply(lambda row: False if row[0] in ['A', 'B','C'] else True), :]\n",
    "y = X['loan_status'].replace({'Defaulted':0, 'FullyPaid':1})\n",
    "X = X.iloc[:, :-1]\n",
    "\n",
    "random_state = 42\n",
    "test_size    = .2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, \n",
    "                                                    random_state=random_state, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding the dataset\n",
    "## Ordinal Encoder\n",
    "cat_columns = ['term', 'home_ownership', 'purpose', 'addr_state', 'initial_list_status']\n",
    "ordinal_columns = ['sub_grade', 'emp_length']\n",
    "\n",
    "sub_grades = sorted(X['sub_grade'].unique())\n",
    "sub_grade_map = {e:i for i, e in enumerate(sub_grades)}\n",
    "\n",
    "emp_lengths = sorted(X['emp_length'].unique())\n",
    "emp_length_map = {e:i for i, e in zip([1, 10, 2, 3, 4, 5, 6, 7, 8, 9, 0], emp_lengths)}\n",
    "\n",
    "\n",
    "mapping=[{'col':'sub_grade' , 'mapping':sub_grade_map}, \n",
    "         {'col':'emp_length', 'mapping':emp_length_map}]\n",
    "\n",
    "for col in cat_columns:\n",
    "    col_map = {e:i for i, e in enumerate(sorted(X[col].unique()))}\n",
    "    mapping.append({'col':col, 'mapping': col_map})\n",
    "\n",
    "ord_encoder = ce.OrdinalEncoder(cols= cat_columns + ordinal_columns, \n",
    "                                mapping=mapping)\n",
    "\n",
    "ord_encoder.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One Hot Encoder\n",
    "cat_columns = ['term', 'home_ownership', 'purpose', 'addr_state', 'initial_list_status']\n",
    "ordinal_columns = ['sub_grade', 'emp_length']\n",
    "\n",
    "one_hot_encoder = ce.OneHotEncoder(cols = cat_columns + ordinal_columns)\n",
    "\n",
    "one_hot_encoder.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ordtrain  = ord_encoder.transform(X_train)\n",
    "X_ordtest   = ord_encoder.transform(X_test)\n",
    "\n",
    "X_ohtrain = one_hot_encoder.transform(X_train)\n",
    "X_ohtest  = one_hot_encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping the first column of the dummification\n",
    "drop_first = ['term_1', 'home_ownership_1', 'purpose_1','addr_state_1',\n",
    "              'initial_list_status_1','sub_grade_1','emp_length_1']\n",
    "X_ohtrain  = X_ohtrain.drop(columns = drop_first)\n",
    "X_ohtest   = X_ohtest.drop(columns = drop_first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reescaling the columns of the one hot encoding\n",
    "std_sc = StandardScaler()\n",
    "std_sc.fit(X_ohtrain, y_train)\n",
    "X_ohtrain = std_sc.transform(X_ohtrain)\n",
    "X_ohtest  = std_sc.transform(X_ohtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the scaling for preprocessing\n",
    "from joblib import dump\n",
    "\n",
    "# dump(ord_encoder,     './models/encoders/ordinal_encoder.joblib')\n",
    "# dump(one_hot_encoder, './models/encoders/one_hot_encoder.joblib')\n",
    "# dump(std_sc,          './models/encoders/standard_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "ord_encoder     = load('./models/encoders/ordinal_encoder.joblib')\n",
    "one_hot_encoder = load('./models/encoders/one_hot_encoder.joblib')\n",
    "std_sc          = load('./models/encoders/standard_scaler.joblib')\n",
    "\n",
    "def preprocessing(df):\n",
    "    df = df.drop(index=df.loc[df['loan_status']=='Current', :].index)\n",
    "    X = df.loc[df['sub_grade'].apply(lambda row: False if row[0] in ['A', 'B','C'] else True), :]\n",
    "    y = X['loan_status'].replace({'Defaulted':0, 'FullyPaid':1})\n",
    "    X = X.iloc[:, :-1]\n",
    "    \n",
    "    X_ord  = ord_encoder.transform(X)\n",
    "\n",
    "    X_oh   = one_hot_encoder.transform(X)\n",
    "\n",
    "    drop_first = ['term_1', 'home_ownership_1', 'purpose_1','addr_state_1',\n",
    "                  'initial_list_status_1','sub_grade_1','emp_length_1']\n",
    "    X_oh  = X_oh.drop(columns = drop_first)\n",
    "    \n",
    "    X_oh   = std_sc.transform(X_oh)\n",
    "    \n",
    "    return X_ord, X_oh, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, confusion_matrix, precision_score, accuracy_score\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    A = confusion_matrix(y_true, y_pred)\n",
    "    print(\n",
    "    f''' \n",
    "    F1 score              | {round(f1_score(y_true, y_pred)*100, 2)}        \\n\n",
    "    -----------------------------\n",
    "    Precision score       | {round(precision_score(y_true, y_pred)*100, 2)} \\n\n",
    "    -----------------------------\n",
    "    Accuracy              | {round(accuracy_score(y_true, y_pred)*100, 2)}  \\n\n",
    "    -----------------------------\n",
    "    Confusion matrix | {A[0,0]} {A[0,1]}\n",
    "                       {A[1,0]} {A[1,1]}\n",
    "    '''\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = keras.Sequential()\n",
    "my_init = keras.initializers.RandomNormal(mean=0.0, stddev=4, seed=None)\n",
    "model_1.add(layers.Input(shape=(146,)))\n",
    "\n",
    "model_1.add(layers.Dense(12, activation = 'tanh', name='dense_1', kernel_regularizer=keras.regularizers.l2(0.001)))\n",
    "\n",
    "model_1.add(layers.Dense(1, activation='sigmoid', name='predictions'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=keras.optimizers.SGD(learning_rate=1e-3), \n",
    "                     loss='binary_crossentropy',\n",
    "                     metrics=['accuracy', keras.metrics.Precision()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()[1]/y_train.value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {0: 2.46, 1:1} ## 2.46 = y_train.value_counts()[1]/y_train.value_counts()[0]\n",
    "\n",
    "model_1.fit(np.array(X_ohtrain), np.array(y_train), epochs=500, batch_size=200,  class_weight=weights, \n",
    "            validation_data=(np.array(X_ohtest), np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1.save('./models/shallowNN_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## All parameters were obtained doing many grid searches. The code for this gridsearch is not here\n",
    "\n",
    "rf_params={'max_depth': 15,\n",
    "           'max_features': 5,\n",
    "           'max_leaf_nodes': None,\n",
    "           'min_impurity_decrease': 0.0,\n",
    "           'min_samples_leaf': 3,\n",
    "           'min_samples_split': 6,\n",
    "           'n_estimators': 2000}\n",
    "\n",
    "xgb_params={'alpha': 0.01,\n",
    "            'gamma': 0,\n",
    "           'max_depth': 11,\n",
    "           'min_child_weight': 8,\n",
    "           'n_estimators': 2200}\n",
    "\n",
    "\n",
    "rf_params2 = {'max_depth': 8,\n",
    "              'max_features': 11,\n",
    "              'max_leaf_nodes': None,\n",
    "              'min_impurity_decrease': 0.0,\n",
    "              'min_samples_leaf': 3,\n",
    "              'min_samples_split': 6,\n",
    "              'n_estimators': 2500}\n",
    "\n",
    "xgb_params2 = {'colsample_bylevel': 0.7,\n",
    "               'max_depth': 7,\n",
    "               'n_estimators': 2500,\n",
    "                'reg_lambda': 1.0,\n",
    "                'scale_pos_weight': 0.6228464256117242,\n",
    "                'subsample': 0.8}\n",
    " \n",
    "lgbm_params =  {'max_depth': 8,\n",
    " 'min_child_samples': 6,\n",
    " 'n_estimators': 2500,\n",
    " 'reg_lambda': 0.67}\n",
    "\n",
    "logit_params = {\n",
    "    'C':2.891111111111111,\n",
    "    'penalty':'l1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import  XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf  = RandomForestClassifier(bootstrap=True, class_weight='balanced', \n",
    "                                   criterion='gini', oob_score=False, **rf_params)\n",
    "\n",
    "neg_pos_ratio = y_train.value_counts()[0]/y_train.value_counts()[1]\n",
    "\n",
    "model_xgb = XGBClassifier(booster = \"gbtree\", objective = \"binary:logistic\",\n",
    "                               lerning_rate = 0.1, n_jobs = -1, scale_pos_weight = neg_pos_ratio,\n",
    "                               **xgb_params)\n",
    "\n",
    "model_logit = LogisticRegression(max_iter=1e6, warm_start=True, class_weight = 'balanced',\n",
    "                                 **logit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf2  = RandomForestClassifier(bootstrap=True, class_weight='balanced', \n",
    "                                   criterion='gini', oob_score=False, **rf_params2)\n",
    "\n",
    "\n",
    "model_xgb2 = XGBClassifier(booster = \"gbtree\", objective = \"binary:logistic\",\n",
    "                               lerning_rate = 0.1, n_jobs = -1,\n",
    "                               **xgb_params2)\n",
    "\n",
    "model_lgbm = LGBMClassifier(learning_rate = 1e-2, class_weight = 'balanced', \n",
    "                            importance_type = 'gain', **lgbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a long time\n",
    "model_rf.fit(X_ordtrain, y_train)\n",
    "model_xgb.fit(X_ordtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf2.fit(X_ordtrain, y_train)\n",
    "model_xgb2.fit(X_ordtrain, y_train)\n",
    "model_lgbm.fit(X_ordtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logit.fit(X_ohtrain, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dump(model_rf, './models/RandomForest_1.joblib')\n",
    "# dump(model_xgb, './models/XGBoost_1.joblib')\n",
    "# dump(model_rf2, './models/RandomForest_2.joblib')\n",
    "# dump(model_xgb2, './models/XGBoost_2.joblib')\n",
    "# dump(model_lgbm, './models/LGBM_1.joblib')\n",
    "# dump(model_logit, './models/logit.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from joblib import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rf    = load('./models/RandomForest_1.joblib')\n",
    "m_xgb   = load('./models/XGBoost_1.joblib')\n",
    "m_rf2   = load('./models/RandomForest_2.joblib')\n",
    "m_xgb2  = load('./models/XGBoost_2.joblib')\n",
    "m_lgbm  = load('./models/LGBM_1.joblib')\n",
    "m_logit = load('./models/logit.joblib')\n",
    "m_nn    = load_model('./models/shallowNN_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ord = [m_rf, m_xgb, m_rf2, m_xgb2, m_lgbm] \n",
    "models_oh  = [m_logit, m_nn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def ensemble(X_ord, X_oh, models_ord, models_oh, treshold = None):\n",
    "    if treshold == None:\n",
    "        treshold = (len(models_ord) + len(models_oh))//2 + 1\n",
    "    predictions = []\n",
    "    for model in models_ord:\n",
    "        predictions.append(pd.Series(model.predict(X_ord)))\n",
    "    for model in models_oh:\n",
    "        predictions.append(pd.Series(model.predict(X_oh).reshape(-1)).apply(lambda x: 1 if x > .5 else 0))\n",
    "    \n",
    "    print(list(map(len, predictions)))\n",
    "    y_sum = reduce(lambda x, y: x+y, predictions)\n",
    "    return y_sum.apply(lambda x: 1 if x >= treshold else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ensemble(X_ordtest, X_ohtest, models_ord, models_oh, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing on other datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2015 =  pd.read_csv('/Users/ivanpassoni/Google Drive/LendingClubData/ml datasets/accepted_2015_ml.csv',\n",
    "                        low_memory = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2015_ord, X_2015_oh, y = preprocessing(test2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = ensemble(X_2015_ord, X_2015_oh, models_ord, models_oh, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base\n",
    "score(y, pd.Series(np.ones(len(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2016 =  pd.read_csv('/Users/ivanpassoni/Google Drive/LendingClubData/ml datasets/accepted_2016_ml.csv',\n",
    "                        low_memory = False)\n",
    "\n",
    "X_2016_ord, X_2016_oh, y_2016 = preprocessing(test2016)\n",
    "\n",
    "y_pred_2016 = ensemble(X_2016_ord, X_2016_oh, models_ord, models_oh, 7)\n",
    "\n",
    "score(y_2016, y_pred_2016)\n",
    "\n",
    "# Base\n",
    "score(y_2016, pd.Series(np.ones(len(y_2016))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
